{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. collect requests & responses by file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/strange_identifiers_9999_completion_items.json, 2808 files\n"
     ]
    }
   ],
   "source": [
    "forget_gap = 9999\n",
    "lsp_reqs_dir = f\"/Users/tannpopo/coding/coding-interfere/strange_identifiers_{forget_gap}_completion_items\"\n",
    "output_file = f\"data/strange_identifiers_{forget_gap}_completion_items.json\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "req_map = {}\n",
    "\n",
    "for lsp_reqs_file in os.listdir(lsp_reqs_dir):\n",
    "    if lsp_reqs_file.endswith(\".json\"):\n",
    "        with open(os.path.join(lsp_reqs_dir, lsp_reqs_file)) as f:\n",
    "            reqs = json.load(f)\n",
    "            for req in reqs:\n",
    "                if req[\"file_path\"] not in req_map:\n",
    "                    req_map[req[\"file_path\"]] = []\n",
    "                req_map[req[\"file_path\"]].append(req)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for file_path, reqs in req_map.items():\n",
    "    results[file_path] = []\n",
    "    for req in reqs:\n",
    "        trigger_point = (req[\"strange_identifier\"][\"end_row\"], req[\"strange_identifier\"][\"end_col\"] - len(req[\"strange_identifier\"][\"name\"]))\n",
    "        trigger_byte = req[\"strange_identifier\"][\"end_byte\"] - len(req[\"strange_identifier\"][\"name\"])\n",
    "        completion_items = [('\"'+item[\"detail\"]+'\"') if \"detail\" in item else item[\"completionText\"] for item in req[\"completion_items\"]]\n",
    "        response = \"[\" + \", \".join(completion_items) + \"]\"\n",
    "        results[file_path].append({\n",
    "            \"request_type\": \"getCompletion\",\n",
    "            \"trigger_point\": trigger_point,\n",
    "            \"trigger_byte\": trigger_byte,\n",
    "            \"response\": response\n",
    "        })\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Output to {output_file}, {len(results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. generate context and completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_in_range(point, start, end):\n",
    "    return start[0] < point[0] < end[0] or ( start[0] < end[0] and ((start[0] == point[0] and start[1] < point[1]) or (point[0] == end[0] and point[1] < end[1]))) or (start[0] == end[0] and start[1] <= point[1] and point[1] <= end[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Population must be a sequence.  For dicts or sets, use sorted(d).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# valid_file_set = random.sample(file_set, int(len(file_set)*0.1))\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m valid_repo_set \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrepo_set\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path, reqs \u001b[38;5;129;01min\u001b[39;00m comp\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# if file_path in valid_file_set:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#     output_file = valid_file\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#     output_file = train_file\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     relative_file_path \u001b[38;5;241m=\u001b[39m file_path[\u001b[38;5;28mlen\u001b[39m(repo_root):]\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/crawler/lib/python3.11/random.py:439\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Sampling without replacement entails tracking either potential\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# selections (the pool) in a list or previous selections in a set.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# too many calls to _randbelow(), making them slower and\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# causing them to eat more entropy than necessary.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(population, _Sequence):\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPopulation must be a sequence.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor dicts or sets, use sorted(d).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    441\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Population must be a sequence.  For dicts or sets, use sorted(d)."
     ]
    }
   ],
   "source": [
    "train_file = f\"data/strange_identifiers_{forget_gap}_dot_training_instances.jsonl\"\n",
    "valid_file = f\"data/strange_identifiers_{forget_gap}_dot_validation_instances.jsonl\"\n",
    "repo_root = \"/Users/tannpopo/coding/coding-interfere/repo_to_mine/\"\n",
    "\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "JA_LANGUAGE = Language(\"tree_sitter_build/language_set.so\", \"java\")\n",
    "parser = Parser()\n",
    "parser.set_language(JA_LANGUAGE)\n",
    "\n",
    "comp_file = \"data/strange_identifiers_9999_completion_items.json\"\n",
    "with open(comp_file) as f:\n",
    "    comp = json.load(f)\n",
    "\n",
    "method_decl_query = JA_LANGUAGE.query(\n",
    "\"\"\"\n",
    "(method_declaration) @method-declaration\n",
    "\"\"\")\n",
    "valid_cnt = 0\n",
    "train_cnt = 0\n",
    "with open(train_file, \"w\") as f:\n",
    "    pass\n",
    "with open(valid_file, \"w\") as f:\n",
    "    pass\n",
    "# file_set = list(comp.keys())\n",
    "repo_set = set()\n",
    "for file_path, reqs in comp.items():\n",
    "    relative_file_path = file_path[len(repo_root):]\n",
    "    repo_name = relative_file_path.split(\"/\")[0]\n",
    "    repo_set.add(repo_name)\n",
    "repo_set = list(repo_set)\n",
    "\n",
    "import random\n",
    "random.seed(16)\n",
    "# valid_file_set = random.sample(file_set, int(len(file_set)*0.1))\n",
    "valid_repo_set = random.sample(repo_set, int(len(repo_set)*0.1))\n",
    "for file_path, reqs in comp.items():\n",
    "    # if file_path in valid_file_set:\n",
    "    #     output_file = valid_file\n",
    "    # else:\n",
    "    #     output_file = train_file\n",
    "    relative_file_path = file_path[len(repo_root):]\n",
    "    repo_name = relative_file_path.split(\"/\")[0]\n",
    "    relative_file = relative_file_path[len(repo_name)+1:]\n",
    "    print(f\"Checking {file_path}\")\n",
    "\n",
    "    if repo_name in valid_repo_set:\n",
    "        output_file = valid_file\n",
    "    else:\n",
    "        output_file = train_file\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        code = f.read()\n",
    "    byte_str = bytes(code, \"utf8\")\n",
    "    tree = parser.parse(byte_str)\n",
    "    method_decls = method_decl_query.captures(tree.root_node)\n",
    "    _method_decls = []\n",
    "\n",
    "    for idx, method_decl in enumerate(method_decls):\n",
    "        if idx == 0:\n",
    "            _method_decls.append(method_decl)\n",
    "            continue\n",
    "        if method_decl[0].end_byte > method_decls[idx-1][0].end_byte:\n",
    "            _method_decls.append(method_decl)\n",
    "        \n",
    "    method_decls = _method_decls\n",
    "\n",
    "    for method_decl in method_decls:\n",
    "        method_decl = method_decl[0]\n",
    "        body = method_decl.child_by_field_name(\"body\")\n",
    "        if body is None:\n",
    "            continue\n",
    "\n",
    "        start_idx = -1\n",
    "        for idx, req in enumerate(reqs):\n",
    "            trigger_point = req[\"trigger_point\"]\n",
    "            if point_in_range(trigger_point, method_decl.start_point, method_decl.end_point):\n",
    "                start_idx = idx\n",
    "                break\n",
    "        \n",
    "        if start_idx == -1:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps({\n",
    "                    \"repo\": repo_name,\n",
    "                    \"file\": relative_file,\n",
    "                    \"context\": byte_str[:body.start_byte+1].decode(\"utf8\"),\n",
    "                    \"completion\": byte_str[body.start_byte+1:method_decl.end_byte].decode(\"utf8\"),\n",
    "                    \"type\": \"no_api_call\",\n",
    "                })+\"\\n\")\n",
    "            if output_file == valid_file:\n",
    "                valid_cnt += 1\n",
    "            else:\n",
    "                train_cnt += 1\n",
    "            continue\n",
    "\n",
    "        context = byte_str[:body.start_byte+1].decode(\"utf8\")\n",
    "        last_end_byte = body.start_byte+1\n",
    "        for idx, req in enumerate(reqs[start_idx:]):\n",
    "            trigger_point = req[\"trigger_point\"]\n",
    "            trigger_byte = req[\"trigger_byte\"]\n",
    "            if point_in_range(trigger_point, method_decl.start_point, method_decl.end_point):\n",
    "                completion = byte_str[last_end_byte:trigger_byte].decode(\"utf8\")\n",
    "                completion += f\"\"\"<request>LSP::getCompletion()\"\"\"\n",
    "                with open(output_file, \"a\") as f:\n",
    "                        f.write(json.dumps({\n",
    "                        \"repo\": repo_name,\n",
    "                        \"file\": relative_file,\n",
    "                        \"context\": context,\n",
    "                        \"completion\": completion,\n",
    "                        \"type\": \"getCompletion:request\" if idx == 0 else \"getCompletion:generationAndRequest\"\n",
    "                    })+\"\\n\")\n",
    "                if output_file == valid_file:\n",
    "                    valid_cnt += 1\n",
    "                else:\n",
    "                    train_cnt += 1\n",
    "                last_end_byte = trigger_byte\n",
    "                context = byte_str[:trigger_byte].decode(\"utf8\")\n",
    "                context += completion+f\"\"\"<response>{req[\"response\"]}</response></request>\"\"\"\n",
    "            else:\n",
    "                # completion = byte_str[last_end_byte:method_decl.end_byte].decode(\"utf8\")\n",
    "                # with open(output_file, \"a\") as f:\n",
    "                #     f.write(json.dumps({\n",
    "                #         \"repo\": repo_name,\n",
    "                #         \"file\": relative_file,\n",
    "                #         \"context\": context,\n",
    "                #         \"completion\": completion,\n",
    "                #         \"type\": \"getCompletion:generation\"\n",
    "                #     })+\"\\n\")\n",
    "                # instance_cnt += 1\n",
    "                break\n",
    "    \n",
    "        completion = byte_str[last_end_byte:method_decl.end_byte].decode(\"utf8\")\n",
    "        with open(output_file, \"a\") as f:\n",
    "            f.write(json.dumps({\n",
    "                \"repo\": repo_name,\n",
    "                \"file\": relative_file,\n",
    "                \"context\": context,\n",
    "                \"completion\": completion,\n",
    "                \"type\": \"getCompletion:generation\"\n",
    "            })+\"\\n\")\n",
    "            if output_file == valid_file:\n",
    "                valid_cnt += 1\n",
    "            else:\n",
    "                train_cnt += 1\n",
    "\n",
    "print(f\"Output to {output_file}, {train_cnt} train instances, {valid_cnt} valid instances\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
