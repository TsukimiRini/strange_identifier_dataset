{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. crawl JAVA repos after 2023.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"java\"\n",
    "page=1\n",
    "date=\"2023-08-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the base URL for the GitHub API\n",
    "base_url = \"https://api.github.com\"\n",
    "\n",
    "# Set the search query parameters\n",
    "query = f\"language:{lang}+created:>{date}&sort=stars&order=desc&per_page=100&page={page}\"\n",
    "\n",
    "# Send a GET request to the search repositories endpoint\n",
    "response = requests.get(f\"{base_url}/search/repositories?q={query}\")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the list of repositories\n",
    "    repositories = data[\"items\"]\n",
    "    \n",
    "    with open(f\"repository_lists/{lang}_{date}_{page}.json\", \"w\") as f:\n",
    "        # Write the list of repositories to the file\n",
    "        json.dump(repositories, f, indent=4)\n",
    "else:\n",
    "    print(\"Error: Failed to retrieve repositories from GitHub\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. clone repos & filter out JAVA repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "for repository in repositories:\n",
    "    # Clone the repository\n",
    "    if not os.path.isdir(f\"repositories/{repository['name']}\"):\n",
    "        os.system(f\"cd repositories;git clone {repository['clone_url']}\")\n",
    "\n",
    "    # log it\n",
    "    logger.info(f\"Cloned {repository['name']}\")\n",
    "    \n",
    "    # check if the repository is a maven project or a gradle project\n",
    "    if not os.path.isfile(f\"repositories/{repository['name']}/pom.xml\") and not os.path.isfile(f\"repositories/{repository['name']}/build.gradle\"):\n",
    "        os.system(f\"rm -rf repositories/{repository['name']}\")\n",
    "        logger.info(f\"Removed {repository['name']} because it is not a maven project or a gradle project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detect Strange Identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build treesitter for java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tree_sitter import Language\n",
    "\n",
    "Language.build_library(\"tree_sitter_build/language_set.so\", [\"tree-sitter-java\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- helper functions to parse Java repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Parser, Language\n",
    "JA_LANGUAGE = Language(\"tree_sitter_build/language_set.so\", \"java\")\n",
    "parser = Parser()\n",
    "parser.set_language(JA_LANGUAGE)\n",
    "\n",
    "class Identifier:\n",
    "    def __init__(self, node, kind=\"identifier\"):\n",
    "        self.node = node\n",
    "        self.name = node.text.decode()\n",
    "        self.full_name = node.text.decode()\n",
    "        self.start_byte = node.start_byte\n",
    "        self.end_byte = node.end_byte\n",
    "        self.start_row = node.start_point[0]\n",
    "        self.end_row = node.end_point[0]\n",
    "        self.start_col = node.start_point[1]\n",
    "        self.end_col = node.end_point[1]\n",
    "        self.kind = kind\n",
    "\n",
    "    def __dict__(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"full_name\": self.full_name,\n",
    "            \"start_byte\": self.start_byte,\n",
    "            \"end_byte\": self.end_byte,\n",
    "            \"start_row\": self.start_row,\n",
    "            \"end_row\": self.end_row,\n",
    "            \"start_col\": self.start_col,\n",
    "            \"end_col\": self.end_col,\n",
    "            \"kind\": self.kind\n",
    "        }\n",
    "        \n",
    "\n",
    "def getParsedTree(src):\n",
    "    bytecodes = bytes(src, \"utf8\")\n",
    "    tree = parser.parse(bytecodes)\n",
    "    return tree\n",
    "\n",
    "def getIdentifier(tree):\n",
    "    for child in tree.root_node.children:\n",
    "        if child.type == \"identifier\":\n",
    "            yield child\n",
    "\n",
    "def getIdentifiersByQuery(tree, queries):\n",
    "    query_str = \"\\n\".join(queries)\n",
    "    query = JA_LANGUAGE.query(query_str)\n",
    "    captures = query.captures(tree.root_node)\n",
    "    return captures\n",
    "\n",
    "tree_obj = None\n",
    "remove_itr = 0\n",
    "def checkOverlappedIdentifier(tree, identifier_list, identifier):\n",
    "    global remove_itr\n",
    "    global tree_obj\n",
    "    if tree != tree_obj:\n",
    "        remove_itr = 0\n",
    "        tree_obj = tree\n",
    "    # print(remove_itr)\n",
    "    for i_idx in range(remove_itr, len(identifier_list)):\n",
    "        i = identifier_list[i_idx]\n",
    "        if i.start_byte <= identifier.start_byte and i.end_byte >= identifier.start_byte or i.start_byte <= identifier.end_byte and i.end_byte >= identifier.end_byte:\n",
    "            remove_itr = i_idx\n",
    "            return True\n",
    "        elif i.start_byte > identifier.end_byte:\n",
    "            remove_itr = i_idx\n",
    "            return False\n",
    "    remove_itr = len(identifier_list)\n",
    "    return False\n",
    "\n",
    "def getLongIdentifiers(tree, byte_str, to_remove):\n",
    "    queries = [\n",
    "        # \"\"\"(method_invocation\n",
    "        #         object: (_) @dot ?\n",
    "        #         name: (_) @dot) @call\"\"\",\n",
    "        # \"\"\"(field_access\n",
    "        #         object: (_) @dot\n",
    "        #         field: (_) @dot) @field_access\"\"\",\n",
    "        \"\"\"(identifier) @identifier\"\"\",\n",
    "    ]\n",
    "\n",
    "    queriedLongIdentifier = getIdentifiersByQuery(tree, queries=queries)\n",
    "    long_identifiers = []\n",
    "    # for idx, long_identifier in enumerate(queriedLongIdentifier):\n",
    "    #     if long_identifier[1] == \"dot\":\n",
    "    #         if queriedLongIdentifier[idx-1][1] != \"dot\":\n",
    "    #             long_identifiers.append(Identifier(long_identifier[0]))\n",
    "    #         else:\n",
    "    #             long_identifiers[-1].name += \".\" + long_identifier[0].text.decode()\n",
    "    #             long_identifiers[-1].end_byte = long_identifier[0].end_byte\n",
    "    #             long_identifiers[-1].end_row = long_identifier[0].end_point[0]\n",
    "    #             long_identifiers[-1].end_col = long_identifier[0].end_point[1]\n",
    "    #     elif long_identifier[1] == \"keep_last\":\n",
    "    #         if queriedLongIdentifier[idx-1][1] != \"keep_last\":\n",
    "    #             long_identifiers.append(Identifier(long_identifier[0]))\n",
    "    #         else:\n",
    "    #             long_identifiers[-1].name = long_identifier[0].text.decode()\n",
    "    #             long_identifiers[-1].end_byte = long_identifier[0].end_byte\n",
    "    #             long_identifiers[-1].end_row = long_identifier[0].end_point[0]\n",
    "    #             long_identifiers[-1].end_col = long_identifier[0].end_point[1]\n",
    "\n",
    "    for idx, identifier in enumerate(queriedLongIdentifier):\n",
    "        if checkOverlappedIdentifier(tree, to_remove, Identifier(identifier[0])):\n",
    "            continue\n",
    "        if idx != 0 and queriedLongIdentifier[idx-1][0].end_byte + 1 == identifier[0].start_byte and byte_str[queriedLongIdentifier[idx-1][0].end_byte] == 46:\n",
    "            long_identifiers[-1].name = identifier[0].text.decode()\n",
    "            long_identifiers[-1].full_name += \".\" + identifier[0].text.decode()\n",
    "            long_identifiers[-1].end_byte = identifier[0].end_byte\n",
    "            long_identifiers[-1].end_row = identifier[0].end_point[0]\n",
    "            long_identifiers[-1].end_col = identifier[0].end_point[1]\n",
    "        else:\n",
    "            long_identifiers.append(Identifier(identifier[0]))\n",
    "\n",
    "    # print([i.name for i in long_identifiers])\n",
    "\n",
    "    return long_identifiers\n",
    "\n",
    "def getDeclaredIdentifiers(tree):\n",
    "    queries = [\n",
    "        \"\"\"(class_declaration\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(method_declaration\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(formal_parameter\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(variable_declarator\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(package_declaration (\n",
    "                scoped_identifier\n",
    "                    scope: (_) @keep_last ?\n",
    "                    name: (_) @keep_last\n",
    "                )) @package\"\"\",\n",
    "        \"\"\"(import_declaration (\n",
    "                scoped_identifier\n",
    "                    scope: (_) @keep_last ?\n",
    "                    name: (_) @keep_last\n",
    "                )) @import\"\"\",\n",
    "        \"\"\"(inferred_parameters\n",
    "                (identifier) @identifier)\"\"\",\n",
    "        \"\"\"(marker_annotation\n",
    "                name: (_) @identifier)\"\"\",\n",
    "    ]\n",
    "\n",
    "    queriedIdentifier = getIdentifiersByQuery(tree, queries=queries)\n",
    "    # print(queriedIdentifier)\n",
    "    identifiers = []\n",
    "    for idx, identifier in enumerate(queriedIdentifier):\n",
    "        if identifier[1] == \"identifier\":\n",
    "            identifiers.append(Identifier(identifier[0], kind=\"declared\"))\n",
    "        elif identifier[1] == \"keep_last\":\n",
    "            if queriedIdentifier[idx-1][1] != \"keep_last\":\n",
    "                identifiers.append(Identifier(identifier[0], kind=\"declared\"))\n",
    "            else:\n",
    "                identifiers[-1].name = identifier[0].text.decode()\n",
    "                identifiers[-1].full_name += \".\" + identifier[0].text.decode()\n",
    "                identifiers[-1].end_byte = identifier[0].end_byte\n",
    "                identifiers[-1].end_row = identifier[0].end_point[0]\n",
    "                identifiers[-1].end_col = identifier[0].end_point[1]\n",
    "        # elif identifier[1] == \"scope\":\n",
    "        #     identifiers.append(Identifier(identifier[0], kind=\"declared\"))\n",
    "    return identifiers\n",
    "\n",
    "def getReferencedIdentifiers(tree, to_remove):\n",
    "    global remove_itr\n",
    "    global tree_obj\n",
    "    if tree != tree_obj:\n",
    "        remove_itr = 0\n",
    "        tree_obj = tree\n",
    "\n",
    "    queries = [\n",
    "        \"\"\"(identifier) @identifier\"\"\",\n",
    "    ]\n",
    "\n",
    "    queriedIdentifier = getIdentifiersByQuery(tree, queries=queries)\n",
    "    identifiers = []\n",
    "    for identifier_idx in range(remove_itr, len(queriedIdentifier)):\n",
    "        identifier = queriedIdentifier[identifier_idx]\n",
    "        if not checkOverlappedIdentifier(tree, to_remove, Identifier(identifier[0])):\n",
    "            identifiers.append(Identifier(identifier[0]))\n",
    "    return identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get identifier lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdentifiers(file):\n",
    "    with open(file, 'r') as f:\n",
    "        src = f.read()\n",
    "    tree = getParsedTree(src)\n",
    "\n",
    "    declared_identifiers = getDeclaredIdentifiers(tree)\n",
    "    # print([i.name for i in declared_identifiers])\n",
    "\n",
    "    long_identifiers = getLongIdentifiers(tree, bytes(src, \"utf8\"), declared_identifiers)\n",
    "\n",
    "    # to_remove = long_identifiers + declared_identifiers\n",
    "    # to_remove = sorted(to_remove, key=lambda x: x.start_byte)\n",
    "\n",
    "    # identifiers = getReferencedIdentifiers(tree, to_remove)\n",
    "\n",
    "    return long_identifiers, declared_identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter out strange identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStrangeIdentifiers(file, forget=9999):\n",
    "    long_identifiers, declared_identifiers = getIdentifiers(file)\n",
    "    # print(file)\n",
    "    # print([i.name for i in long_identifiers])\n",
    "    # print([i.name for i in declared_identifiers])\n",
    "\n",
    "    identifier_map = {}\n",
    "\n",
    "    all_identifiers = long_identifiers + declared_identifiers\n",
    "    all_identifiers = sorted(all_identifiers, key=lambda x: x.start_byte)\n",
    "    for identifier in all_identifiers:\n",
    "        if identifier.name not in identifier_map:\n",
    "            identifier_map[identifier.name] = []\n",
    "        identifier_map[identifier.name].append(identifier)\n",
    "\n",
    "    strange_identifiers = []\n",
    "    for identifier in identifier_map:\n",
    "        for idx, inst in enumerate(identifier_map[identifier]):\n",
    "            if inst.kind == \"declared\":\n",
    "                continue\n",
    "            if idx == 0:\n",
    "                strange_identifiers.append(inst)\n",
    "            elif inst.end_row - identifier_map[identifier][idx-1].end_row > forget:\n",
    "                strange_identifiers.append(inst)\n",
    "\n",
    "    # print([i.name for i in strange_identifiers])\n",
    "            \n",
    "    return strange_identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- traverse all repos and detect strange identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_dir = \"/Users/tannpopo/coding/coding-interfere/repo_to_mine\"\n",
    "forget = 9999\n",
    "output_dir = f\"/Users/tannpopo/coding/coding-interfere/strange_identifiers_{forget}\" \n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from loguru import logger\n",
    "import random\n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "def getAllJavaFiles(repo_dir):\n",
    "    java_files = []\n",
    "    for root, dirs, files in os.walk(repo_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".java\"):\n",
    "                java_files.append(os.path.join(root, file))\n",
    "\n",
    "    java_files = random.sample(java_files, min(len(java_files), 200))\n",
    "    return java_files\n",
    "\n",
    "def getStrangeIdentifiersInRepo(repo_dir, forget=9999):\n",
    "    strange_identifiers = []\n",
    "    java_files = getAllJavaFiles(repo_dir)\n",
    "    for file in tqdm(java_files):\n",
    "        cur_identifiers = getStrangeIdentifiers(file, forget=forget)\n",
    "        for idx, identifier in enumerate(cur_identifiers):\n",
    "            strange_identifiers.append({\n",
    "                \"file_path\": file,\n",
    "                \"strange_identifier\": identifier.__dict__(),\n",
    "            })\n",
    "    return strange_identifiers\n",
    "\n",
    "# identifier_test = getStrangeIdentifiers(\"/Users/tannpopo/coding/coding-interfere/repo_to_mine/daydayEXP/src/main/java/com/bcvgh/core/BaseTemplate.java\")\n",
    "# print([i.__dict__() for i in identifier_test])\n",
    "\n",
    "repo_list = os.listdir(repos_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "all_cnt = 0\n",
    "for repository_dir in repo_list:\n",
    "    # if os.path.isfile(f\"{output_dir}/{repository_dir}.json\"):\n",
    "    #     continue\n",
    "    logger.info(f\"Start {repository_dir}\")\n",
    "    if not os.path.isdir(os.path.join(repos_dir, repository_dir)):\n",
    "        continue\n",
    "    strange_identifiers = getStrangeIdentifiersInRepo(os.path.join(repos_dir, repository_dir), forget=forget)\n",
    "    logger.info(f\"Finished {repository_dir}, {len(strange_identifiers)} strange identifiers found\")\n",
    "    all_cnt += len(strange_identifiers)\n",
    "    with open(f\"{output_dir}/{repository_dir}.json\", \"w\") as f:\n",
    "        json.dump(strange_identifiers, f, indent=4)\n",
    "\n",
    "logger.info(f\"Finished all, {all_cnt} strange identifiers found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
