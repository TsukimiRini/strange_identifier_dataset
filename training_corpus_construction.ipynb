{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. crawl JAVA repos after 2023.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"java\"\n",
    "page=1\n",
    "date=\"2023-08-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the base URL for the GitHub API\n",
    "base_url = \"https://api.github.com\"\n",
    "\n",
    "# Set the search query parameters\n",
    "query = f\"language:{lang}+created:>{date}&sort=stars&order=desc&per_page=100&page={page}\"\n",
    "\n",
    "# Send a GET request to the search repositories endpoint\n",
    "response = requests.get(f\"{base_url}/search/repositories?q={query}\")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the list of repositories\n",
    "    repositories = data[\"items\"]\n",
    "    \n",
    "    with open(f\"repository_lists/{lang}_{date}_{page}.json\", \"w\") as f:\n",
    "        # Write the list of repositories to the file\n",
    "        json.dump(repositories, f, indent=4)\n",
    "else:\n",
    "    print(\"Error: Failed to retrieve repositories from GitHub\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. clone repos & filter out JAVA repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "for repository in repositories:\n",
    "    # Clone the repository\n",
    "    if not os.path.isdir(f\"repositories/{repository['name']}\"):\n",
    "        os.system(f\"cd repositories;git clone {repository['clone_url']}\")\n",
    "\n",
    "    # log it\n",
    "    logger.info(f\"Cloned {repository['name']}\")\n",
    "    \n",
    "    # check if the repository is a maven project or a gradle project\n",
    "    if not os.path.isfile(f\"repositories/{repository['name']}/pom.xml\") and not os.path.isfile(f\"repositories/{repository['name']}/build.gradle\"):\n",
    "        os.system(f\"rm -rf repositories/{repository['name']}\")\n",
    "        logger.info(f\"Removed {repository['name']} because it is not a maven project or a gradle project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detect Strange Identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build treesitter for java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tree_sitter import Language\n",
    "\n",
    "Language.build_library(\"tree_sitter_build/language_set.so\", [\"tree-sitter-java\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- helper functions to parse Java repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Parser, Language\n",
    "JA_LANGUAGE = Language(\"tree_sitter_build/language_set.so\", \"java\")\n",
    "parser = Parser()\n",
    "parser.set_language(JA_LANGUAGE)\n",
    "\n",
    "class Identifier:\n",
    "    def __init__(self, node, kind=\"identifier\"):\n",
    "        self.node = node\n",
    "        self.name = node.text.decode()\n",
    "        self.full_name = node.text.decode()\n",
    "        self.start_byte = node.start_byte\n",
    "        self.end_byte = node.end_byte\n",
    "        self.start_row = node.start_point[0]\n",
    "        self.end_row = node.end_point[0]\n",
    "        self.start_col = node.start_point[1]\n",
    "        self.end_col = node.end_point[1]\n",
    "        self.kind = kind\n",
    "\n",
    "    def __dict__(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"full_name\": self.full_name,\n",
    "            \"start_byte\": self.start_byte,\n",
    "            \"end_byte\": self.end_byte,\n",
    "            \"start_row\": self.start_row,\n",
    "            \"end_row\": self.end_row,\n",
    "            \"start_col\": self.start_col,\n",
    "            \"end_col\": self.end_col,\n",
    "            \"kind\": self.kind\n",
    "        }\n",
    "        \n",
    "\n",
    "def getParsedTree(src):\n",
    "    bytecodes = bytes(src, \"utf8\")\n",
    "    tree = parser.parse(bytecodes)\n",
    "    return tree\n",
    "\n",
    "def getIdentifier(tree):\n",
    "    for child in tree.root_node.children:\n",
    "        if child.type == \"identifier\":\n",
    "            yield child\n",
    "\n",
    "def getIdentifiersByQuery(tree, queries):\n",
    "    query_str = \"\\n\".join(queries)\n",
    "    query = JA_LANGUAGE.query(query_str)\n",
    "    captures = query.captures(tree.root_node)\n",
    "    return captures\n",
    "\n",
    "tree_obj = None\n",
    "remove_itr = 0\n",
    "def checkOverlappedIdentifier(tree, identifier_list, identifier):\n",
    "    global remove_itr\n",
    "    global tree_obj\n",
    "    if tree != tree_obj:\n",
    "        remove_itr = 0\n",
    "        tree_obj = tree\n",
    "    # print(remove_itr)\n",
    "    for i_idx in range(remove_itr, len(identifier_list)):\n",
    "        i = identifier_list[i_idx]\n",
    "        if i.start_byte <= identifier.start_byte and i.end_byte >= identifier.start_byte or i.start_byte <= identifier.end_byte and i.end_byte >= identifier.end_byte:\n",
    "            remove_itr = i_idx\n",
    "            return True\n",
    "        elif i.start_byte > identifier.end_byte:\n",
    "            remove_itr = i_idx\n",
    "            return False\n",
    "    remove_itr = len(identifier_list)\n",
    "    return False\n",
    "\n",
    "def getLongIdentifiers(tree, byte_str, to_remove):\n",
    "    queries = [\n",
    "        # \"\"\"(method_invocation\n",
    "        #         object: (_) @dot ?\n",
    "        #         name: (_) @dot) @call\"\"\",\n",
    "        # \"\"\"(field_access\n",
    "        #         object: (_) @dot\n",
    "        #         field: (_) @dot) @field_access\"\"\",\n",
    "        \"\"\"(identifier) @identifier\"\"\",\n",
    "    ]\n",
    "\n",
    "    queriedLongIdentifier = getIdentifiersByQuery(tree, queries=queries)\n",
    "    long_identifiers = []\n",
    "    # for idx, long_identifier in enumerate(queriedLongIdentifier):\n",
    "    #     if long_identifier[1] == \"dot\":\n",
    "    #         if queriedLongIdentifier[idx-1][1] != \"dot\":\n",
    "    #             long_identifiers.append(Identifier(long_identifier[0]))\n",
    "    #         else:\n",
    "    #             long_identifiers[-1].name += \".\" + long_identifier[0].text.decode()\n",
    "    #             long_identifiers[-1].end_byte = long_identifier[0].end_byte\n",
    "    #             long_identifiers[-1].end_row = long_identifier[0].end_point[0]\n",
    "    #             long_identifiers[-1].end_col = long_identifier[0].end_point[1]\n",
    "    #     elif long_identifier[1] == \"keep_last\":\n",
    "    #         if queriedLongIdentifier[idx-1][1] != \"keep_last\":\n",
    "    #             long_identifiers.append(Identifier(long_identifier[0]))\n",
    "    #         else:\n",
    "    #             long_identifiers[-1].name = long_identifier[0].text.decode()\n",
    "    #             long_identifiers[-1].end_byte = long_identifier[0].end_byte\n",
    "    #             long_identifiers[-1].end_row = long_identifier[0].end_point[0]\n",
    "    #             long_identifiers[-1].end_col = long_identifier[0].end_point[1]\n",
    "\n",
    "    for idx, identifier in enumerate(queriedLongIdentifier):\n",
    "        if checkOverlappedIdentifier(tree, to_remove, Identifier(identifier[0])):\n",
    "            continue\n",
    "        if idx != 0 and queriedLongIdentifier[idx-1][0].end_byte + 1 == identifier[0].start_byte and byte_str[queriedLongIdentifier[idx-1][0].end_byte] == 46:\n",
    "            long_identifiers[-1].name = identifier[0].text.decode()\n",
    "            long_identifiers[-1].full_name += \".\" + identifier[0].text.decode()\n",
    "            long_identifiers[-1].end_byte = identifier[0].end_byte\n",
    "            long_identifiers[-1].end_row = identifier[0].end_point[0]\n",
    "            long_identifiers[-1].end_col = identifier[0].end_point[1]\n",
    "        else:\n",
    "            long_identifiers.append(Identifier(identifier[0]))\n",
    "\n",
    "    # print([i.name for i in long_identifiers])\n",
    "\n",
    "    return long_identifiers\n",
    "\n",
    "def getDeclaredIdentifiers(tree):\n",
    "    queries = [\n",
    "        \"\"\"(class_declaration\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(method_declaration\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(formal_parameter\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(variable_declarator\n",
    "                name: (_) @identifier)\"\"\",\n",
    "        \"\"\"(package_declaration (\n",
    "                scoped_identifier\n",
    "                    scope: (_) @keep_last ?\n",
    "                    name: (_) @keep_last\n",
    "                )) @package\"\"\",\n",
    "        \"\"\"(import_declaration (\n",
    "                scoped_identifier\n",
    "                    scope: (_) @keep_last ?\n",
    "                    name: (_) @keep_last\n",
    "                )) @import\"\"\",\n",
    "        \"\"\"(inferred_parameters\n",
    "                (identifier) @identifier)\"\"\",\n",
    "        \"\"\"(marker_annotation\n",
    "                name: (_) @identifier)\"\"\",\n",
    "    ]\n",
    "\n",
    "    queriedIdentifier = getIdentifiersByQuery(tree, queries=queries)\n",
    "    # print(queriedIdentifier)\n",
    "    identifiers = []\n",
    "    for idx, identifier in enumerate(queriedIdentifier):\n",
    "        if identifier[1] == \"identifier\":\n",
    "            identifiers.append(Identifier(identifier[0], kind=\"declared\"))\n",
    "        elif identifier[1] == \"keep_last\":\n",
    "            if queriedIdentifier[idx-1][1] != \"keep_last\":\n",
    "                identifiers.append(Identifier(identifier[0], kind=\"declared\"))\n",
    "            else:\n",
    "                identifiers[-1].name = identifier[0].text.decode()\n",
    "                identifiers[-1].full_name += \".\" + identifier[0].text.decode()\n",
    "                identifiers[-1].end_byte = identifier[0].end_byte\n",
    "                identifiers[-1].end_row = identifier[0].end_point[0]\n",
    "                identifiers[-1].end_col = identifier[0].end_point[1]\n",
    "        # elif identifier[1] == \"scope\":\n",
    "        #     identifiers.append(Identifier(identifier[0], kind=\"declared\"))\n",
    "    return identifiers\n",
    "\n",
    "def getReferencedIdentifiers(tree, to_remove):\n",
    "    global remove_itr\n",
    "    global tree_obj\n",
    "    if tree != tree_obj:\n",
    "        remove_itr = 0\n",
    "        tree_obj = tree\n",
    "\n",
    "    queries = [\n",
    "        \"\"\"(identifier) @identifier\"\"\",\n",
    "    ]\n",
    "\n",
    "    queriedIdentifier = getIdentifiersByQuery(tree, queries=queries)\n",
    "    identifiers = []\n",
    "    for identifier_idx in range(remove_itr, len(queriedIdentifier)):\n",
    "        identifier = queriedIdentifier[identifier_idx]\n",
    "        if not checkOverlappedIdentifier(tree, to_remove, Identifier(identifier[0])):\n",
    "            identifiers.append(Identifier(identifier[0]))\n",
    "    return identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get identifier lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdentifiers(file):\n",
    "    with open(file, 'r') as f:\n",
    "        src = f.read()\n",
    "    tree = getParsedTree(src)\n",
    "\n",
    "    declared_identifiers = getDeclaredIdentifiers(tree)\n",
    "    # print([i.name for i in declared_identifiers])\n",
    "\n",
    "    long_identifiers = getLongIdentifiers(tree, bytes(src, \"utf8\"), declared_identifiers)\n",
    "\n",
    "    # to_remove = long_identifiers + declared_identifiers\n",
    "    # to_remove = sorted(to_remove, key=lambda x: x.start_byte)\n",
    "\n",
    "    # identifiers = getReferencedIdentifiers(tree, to_remove)\n",
    "\n",
    "    return long_identifiers, declared_identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter out strange identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStrangeIdentifiers(file, forget=9999):\n",
    "    long_identifiers, declared_identifiers = getIdentifiers(file)\n",
    "    # print(file)\n",
    "    # print([i.name for i in long_identifiers])\n",
    "    # print([i.name for i in declared_identifiers])\n",
    "\n",
    "    identifier_map = {}\n",
    "\n",
    "    all_identifiers = long_identifiers + declared_identifiers\n",
    "    all_identifiers = sorted(all_identifiers, key=lambda x: x.start_byte)\n",
    "    for identifier in all_identifiers:\n",
    "        if identifier.name not in identifier_map:\n",
    "            identifier_map[identifier.name] = []\n",
    "        identifier_map[identifier.name].append(identifier)\n",
    "\n",
    "    strange_identifiers = []\n",
    "    for identifier in identifier_map:\n",
    "        for idx, inst in enumerate(identifier_map[identifier]):\n",
    "            if inst.kind == \"declared\":\n",
    "                continue\n",
    "            if idx == 0:\n",
    "                strange_identifiers.append(inst)\n",
    "            elif inst.end_row - identifier_map[identifier][idx-1].end_row > forget:\n",
    "                strange_identifiers.append(inst)\n",
    "\n",
    "    # print([i.name for i in strange_identifiers])\n",
    "            \n",
    "    return strange_identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- traverse all repos and detect strange identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_dir = \"/Users/tannpopo/coding/coding-interfere/repo_to_mine\"\n",
    "forget = 9999\n",
    "output_dir = f\"/Users/tannpopo/coding/coding-interfere/strange_identifiers_{forget}\" \n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from loguru import logger\n",
    "import random\n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "def getAllJavaFiles(repo_dir):\n",
    "    java_files = []\n",
    "    for root, dirs, files in os.walk(repo_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".java\"):\n",
    "                java_files.append(os.path.join(root, file))\n",
    "\n",
    "    java_files = random.sample(java_files, min(len(java_files), 200))\n",
    "    return java_files\n",
    "\n",
    "def getStrangeIdentifiersInRepo(repo_dir, forget=9999):\n",
    "    strange_identifiers = []\n",
    "    java_files = getAllJavaFiles(repo_dir)\n",
    "    for file in tqdm(java_files):\n",
    "        cur_identifiers = getStrangeIdentifiers(file, forget=forget)\n",
    "        for idx, identifier in enumerate(cur_identifiers):\n",
    "            strange_identifiers.append({\n",
    "                \"file_path\": file,\n",
    "                \"strange_identifier\": identifier.__dict__(),\n",
    "            })\n",
    "    return strange_identifiers\n",
    "\n",
    "# identifier_test = getStrangeIdentifiers(\"/Users/tannpopo/coding/coding-interfere/repo_to_mine/daydayEXP/src/main/java/com/bcvgh/core/BaseTemplate.java\")\n",
    "# print([i.__dict__() for i in identifier_test])\n",
    "\n",
    "repo_list = os.listdir(repos_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "all_cnt = 0\n",
    "for repository_dir in repo_list:\n",
    "    # if os.path.isfile(f\"{output_dir}/{repository_dir}.json\"):\n",
    "    #     continue\n",
    "    logger.info(f\"Start {repository_dir}\")\n",
    "    if not os.path.isdir(os.path.join(repos_dir, repository_dir)):\n",
    "        continue\n",
    "    strange_identifiers = getStrangeIdentifiersInRepo(os.path.join(repos_dir, repository_dir), forget=forget)\n",
    "    logger.info(f\"Finished {repository_dir}, {len(strange_identifiers)} strange identifiers found\")\n",
    "    all_cnt += len(strange_identifiers)\n",
    "    with open(f\"{output_dir}/{repository_dir}.json\", \"w\") as f:\n",
    "        json.dump(strange_identifiers, f, indent=4)\n",
    "\n",
    "logger.info(f\"Finished all, {all_cnt} strange identifiers found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. create dot training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Task <Task pending name='Task-5' coro=<_AsyncGeneratorContextManager.__aenter__() running at /Users/tannpopo/Env/anaconda3/envs/pytorch/lib/python3.9/contextlib.py:175> cb=[_chain_future.<locals>._call_set_state() at /Users/tannpopo/Env/anaconda3/envs/pytorch/lib/python3.9/asyncio/futures.py:391]> got Future <Future pending> attached to a different loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m                 strange_identifier[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_items\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(completion_item\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m())\n\u001b[1;32m     35\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend(strange_identifier)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mbuildForRepo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdaydayEXP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m, in \u001b[0;36mbuildForRepo\u001b[0;34m(repo)\u001b[0m\n\u001b[1;32m     23\u001b[0m lsp \u001b[38;5;241m=\u001b[39m SyncLanguageServer\u001b[38;5;241m.\u001b[39mcreate(config, logger, repo_path)\n\u001b[1;32m     24\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lsp\u001b[38;5;241m.\u001b[39mstart_server():\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m strange_identifier \u001b[38;5;129;01min\u001b[39;00m tqdm(strange_identifiers):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m strange_identifier[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/contextlib.py:117\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/site-packages/monitors4codegen/multilspy/language_server.py:720\u001b[0m, in \u001b[0;36mSyncLanguageServer.start_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m loop_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    719\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_server\u001b[38;5;241m.\u001b[39mstart_server()\n\u001b[0;32m--> 720\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_coroutine_threadsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__aenter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    722\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m), loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop)\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/concurrent/futures/_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/contextlib.py:175\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/site-packages/monitors4codegen/multilspy/language_servers/eclipse_jdtls/eclipse_jdtls.py:387\u001b[0m, in \u001b[0;36mEclipseJDTLS.start_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mnotify\u001b[38;5;241m.\u001b[39minitialized({})\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mnotify\u001b[38;5;241m.\u001b[39mworkspace_did_change_configuration(\n\u001b[1;32m    384\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m: initialize_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitializationOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    385\u001b[0m )\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintellicode_enable_command_available\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    389\u001b[0m java_intellisense_members_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_dependency_paths\u001b[38;5;241m.\u001b[39mintellisense_members_path\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(java_intellisense_members_path)\n",
      "File \u001b[0;32m~/Env/anaconda3/envs/pytorch/lib/python3.9/asyncio/locks.py:226\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Task <Task pending name='Task-5' coro=<_AsyncGeneratorContextManager.__aenter__() running at /Users/tannpopo/Env/anaconda3/envs/pytorch/lib/python3.9/contextlib.py:175> cb=[_chain_future.<locals>._call_set_state() at /Users/tannpopo/Env/anaconda3/envs/pytorch/lib/python3.9/asyncio/futures.py:391]> got Future <Future pending> attached to a different loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from monitors4codegen.multilspy import SyncLanguageServer\n",
    "from monitors4codegen.multilspy.multilspy_config import MultilspyConfig\n",
    "from monitors4codegen.multilspy.multilspy_logger import MultilspyLogger\n",
    "\n",
    "forget_gap = 9999\n",
    "repos_dir = \"/Users/tannpopo/coding/coding-interfere/repo_to_mine\"\n",
    "strange_identifiers_dir = f\"/Users/tannpopo/coding/coding-interfere/strange_identifiers_{forget_gap}\"\n",
    "\n",
    "config = MultilspyConfig.from_dict({\"code_language\": \"java\"}) # Also supports \"python\", \"rust\", \"csharp\"\n",
    "logger = MultilspyLogger()\n",
    "\n",
    "def readInStrangeIdentifiers(repo):\n",
    "    with open(f\"{strange_identifiers_dir}/{repo}.json\", \"r\") as f:\n",
    "        strange_identifiers = json.load(f)\n",
    "    return strange_identifiers\n",
    "\n",
    "def buildForRepo(repo):\n",
    "    repo_path = os.path.join(repos_dir, repo)\n",
    "    strange_identifiers = readInStrangeIdentifiers(repo)\n",
    "    lsp = SyncLanguageServer.create(config, logger, repo_path)\n",
    "    results = []\n",
    "    with lsp.start_server():\n",
    "        for strange_identifier in tqdm(strange_identifiers):\n",
    "            if \".\" not in strange_identifier[\"full_name\"]:\n",
    "                continue\n",
    "            completion_items = lsp.request_completion(strange_identifier[\"file_path\"], strange_identifier[\"strange_identifier\"][\"start_row\"], strange_identifier[\"strange_identifier\"][\"start_col\"]+len(strange_identifier[\"strange_identifier\"][\"full_name\"])-len(strange_identifier[\"strange_identifier\"][\"name\"])-1)\n",
    "            if completion_items is None or completion_items == []:\n",
    "                continue\n",
    "            strange_identifier[\"completion_items\"] = []\n",
    "            for completion_item in completion_items:\n",
    "                strange_identifier[\"completion_items\"].append(completion_item.__dict__())\n",
    "            results.append(strange_identifier)\n",
    "\n",
    "buildForRepo(\"daydayEXP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
